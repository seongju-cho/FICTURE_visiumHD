{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this script is for\n",
    "### select kernel named as FICTURE\n",
    "\n",
    "import spatialdata as sd\n",
    "import spatialdata_io as sd_io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "visiumHD_output = '/home/sjcho/projects/ATLAS_visiumHD/lung_HN00227429_20241003/1.preprocessing/Lung_tumor_HN00227429/outs'\n",
    "dataset_id = 'Lung_Tumor_HN00227429'\n",
    "spatula_output = '/home/sjcho/projects/ATLAS_visiumHD/lung_HN00227429_20241003/5.spatula/out/5.2.run_spatula/HN00227429_Lung_Tumor/test.tsv.gz'\n",
    "\n",
    "save_path = '/home/sjcho/projects/ATLAS_visiumHD/lung_HN00227429_20241003/6.merge_factor_manually/outs/6.1.merge_spot/'\n",
    "sample_name = 'HN00227429_Lung_Tumor_12um'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add _ in front of the file name\n",
    "# but why this is need? I don't know\n",
    "# maybe only the spatial data developers knows\n",
    "if not os.path.exists(visiumHD_output + '/_feature_slice.h5'):\n",
    "    os.rename(visiumHD_output + '/feature_slice.h5', visiumHD_output + '/_feature_slice.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjcho/.conda/envs/FICTURE/lib/python3.11/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/tmp/ipykernel_50146/1601477502.py:1: UserWarning: No full resolution image found. If incorrect, please specify the path in the `fullres_image_file` parameter when calling the `visium_hd` reader function.\n",
      "  sd = sd_io.visium_hd(path = visiumHD_output, bin_size = 2)\n"
     ]
    }
   ],
   "source": [
    "sd = sd_io.visium_hd(path = visiumHD_output, bin_size = 2, dataset_id=dataset_id)\n",
    "# becuase sd_io.visium_hd try to open feature_slice.h5 in outs folder with prefix of 'dataset_id + _\n",
    "# so.... because of _, please change feature_slice.h5 to dataset_id + _ + feature_slice.h5\n",
    "# or simply add _ in front of feature_slice.h5 and set dataset_id = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50146/2432565994.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  coordinates.drop('#barcode_idx', axis=1, inplace=True)\n",
      "/tmp/ipykernel_50146/2432565994.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  factors.drop('#barcode_idx', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "spatula_out = pd.read_csv(spatula_output, compression='gzip', sep='\\t')\n",
    "spatula_out.columns = ['#barcode_idx', 'X', 'Y', 'factor', 'assign_probability']\n",
    "spatula_out.index = spatula_out['#barcode_idx']\n",
    "spatula_out = spatula_out.loc[sd.tables['square_002um'].obs.index] # only remain barcodes in filtered matrix\n",
    "spatula_out = spatula_out.sort_values(['Y', 'X']) # sort  by Y then X (X is for tie break)\n",
    "\n",
    "coordinates = spatula_out[['#barcode_idx', 'X', 'Y']]\n",
    "coordinates.index = coordinates['#barcode_idx']\n",
    "coordinates.drop('#barcode_idx', axis=1, inplace=True)\n",
    "\n",
    "factors = spatula_out[['#barcode_idx', 'factor', 'assign_probability']]\n",
    "factors.index = factors['#barcode_idx']\n",
    "factors.drop('#barcode_idx', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted index\n",
    "sorted_idx = factors.index\n",
    "\n",
    "# sort sparse count matrix\n",
    "reordered_X = sd.tables['square_002um'].X[sd.tables['square_002um'].obs.index.get_indexer(sorted_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "def process_kernel_batch(batch_data, expression_mat, count_features, point_assignments, unique_integers, coordinates):\n",
    "    \"\"\"\n",
    "    Process a batch of kernels at once to reduce overhead\n",
    "    \"\"\"\n",
    "    start_idx, kernel_batch = batch_data\n",
    "    batch_results = []\n",
    "    \n",
    "    for i, kernel in enumerate(kernel_batch):\n",
    "        mask = (point_assignments == kernel).all(axis=1)\n",
    "        \n",
    "        if not np.any(mask):\n",
    "            batch_results.append((start_idx + i, None, None, None))\n",
    "            continue\n",
    "            \n",
    "        # Calculate sum for expression_mat\n",
    "        expression_sum = np.sum(expression_mat[mask], axis=0)\n",
    "        \n",
    "        # Get kernel values and calculate counts\n",
    "        kernel_values = count_features[mask]\n",
    "        counts = np.zeros(len(unique_integers) + 2)  # +2 for NaN and grid count\n",
    "        counts[0] = np.sum(np.isnan(kernel_values))\n",
    "        \n",
    "        # Vectorized counting for all unique integers\n",
    "        for j, val in enumerate(unique_integers):\n",
    "            counts[j + 1] = np.sum(kernel_values == val)\n",
    "\n",
    "        counts[-1] = np.sum(mask)\n",
    "        # Calculate average of actual coordinates instead of grid positions\n",
    "        kernel_coord = np.mean(coordinates[mask], axis=0)\n",
    "        \n",
    "        batch_results.append((start_idx + i, expression_sum, counts, kernel_coord))\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "def aggregate_features_with_counts_batch(coordinates, expression_mat, count_features, \n",
    "                                       kernel_size=12, batch_size=5000, n_cores=None, \n",
    "                                       show_progress=True):\n",
    "    \"\"\"\n",
    "    Batch-processed version of feature aggregation\n",
    "    \n",
    "    Parameters:\n",
    "    - coordinates: Nx2 array of (x,y) coordinates\n",
    "    - expression_mat: NxF matrix for expression\n",
    "    - count_features: Nx1 array for FICTURE output. containing integers and NaN values to be counted.\n",
    "    - kernel_size: size of the square kernel (default: 6)\n",
    "    - batch_size: number of kernels to process in each batch (default: 100)\n",
    "    - n_cores: number of CPU cores to use (default: None, uses all available cores)\n",
    "    - show_progress: whether to show progress bar (default: True)\n",
    "    \"\"\"\n",
    "    # Get unique integer values (excluding NaN)\n",
    "    unique_integers = np.unique(count_features[~np.isnan(count_features)]).astype(int)\n",
    "    unique_integers.sort()\n",
    "    \n",
    "    # Find spatial bounds\n",
    "    x_min, y_min = np.min(coordinates, axis=0)\n",
    "    x_max, y_max = np.max(coordinates, axis=0)\n",
    "    \n",
    "    # Assign points to kernels using vectorized operations\n",
    "    point_assignments = np.floor((coordinates - [x_min, y_min]) / kernel_size).astype(int)\n",
    "    unique_kernels = np.unique(point_assignments, axis=0)\n",
    "    \n",
    "    # Initialize output arrays\n",
    "    n_kernels = len(unique_kernels)\n",
    "    aggregated_expression_mat = np.zeros((n_kernels, expression_mat.shape[1]))\n",
    "    count_features_dist = np.zeros((n_kernels, len(unique_integers) + 2)) # +2 for NaN and grid count\n",
    "    kernel_coordinates = np.zeros((n_kernels, 2))\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"Processing {n_kernels} kernels in batches of {batch_size} using {n_cores if n_cores else 'all available'} cores...\")\n",
    "    \n",
    "    # Create batches\n",
    "    n_batches = (n_kernels + batch_size - 1) // batch_size\n",
    "    batches = []\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, n_kernels)\n",
    "        batches.append((start_idx, unique_kernels[start_idx:end_idx]))\n",
    "    \n",
    "    # Prepare partial function with fixed arguments\n",
    "    process_func = partial(process_kernel_batch,\n",
    "                         expression_mat=expression_mat,\n",
    "                         count_features=count_features,\n",
    "                         point_assignments=point_assignments,\n",
    "                         unique_integers=unique_integers,\n",
    "                         coordinates=coordinates)  # Added coordinates parameter\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    with Pool(processes=n_cores) as pool:\n",
    "        # Create iterator with progress bar if requested\n",
    "        if show_progress:\n",
    "            iterator = tqdm(batches, total=n_batches, desc=\"Processing kernel batches\")\n",
    "        else:\n",
    "            iterator = batches\n",
    "        \n",
    "        # Process all batches\n",
    "        all_results = pool.imap(process_func, iterator)\n",
    "        \n",
    "        # Collect results\n",
    "        for batch_results in all_results:\n",
    "            for i, expression_sum, counts, kernel_coord in batch_results:\n",
    "                if expression_sum is not None:\n",
    "                    aggregated_expression_mat[i] = expression_sum\n",
    "                    count_features_dist[i] = counts\n",
    "                    kernel_coordinates[i] = kernel_coord\n",
    "    \n",
    "    if show_progress:\n",
    "        print(\"Processing complete!\")\n",
    "    \n",
    "    return aggregated_expression_mat, count_features_dist, kernel_coordinates, ['NaN'] + list(unique_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 250669 kernels in batches of 5000 using 64 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing kernel batches: 100%|██████████| 51/51 [07:39<00:00,  9.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "agg_mean, count_dist, kernel_coords, value_labels = aggregate_features_with_counts_batch(\n",
    "    coordinates, reordered_X, factors.iloc[:, 0].values, kernel_size=12,\n",
    "    show_progress=True, n_cores = 64, batch_size=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250669, 18085)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene names\n",
    "sd.tables['square_002um'].var.loc[:, 'gene_ids'].to_csv(save_path + sample_name + '_gene_names.csv', sep=',', header=True, index=True)\n",
    "\n",
    "# save merged expression matrix\n",
    "sio.mmwrite(save_path + sample_name + '_merged_exp.csv', sparse.csr_matrix(agg_mean))\n",
    "\n",
    "# save mean coordinates\n",
    "np.savetxt(save_path + sample_name + '_mean_coordinates.csv', kernel_coords, delimiter=',')\n",
    "\n",
    "# save count distribution\n",
    "# NaN is first column, then 0, 1, 2, 3, ...\n",
    "np.savetxt(save_path + sample_name + '_count_distribution.csv', count_dist, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_annotation_st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
